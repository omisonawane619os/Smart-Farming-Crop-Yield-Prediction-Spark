{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omisonawane619os/Smart-Farming-Crop-Yield-Prediction-Spark/blob/main/Smart_Farming_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8846bfea-91e9-440e-b34c-7dc2a54b277e",
      "metadata": {
        "id": "8846bfea-91e9-440e-b34c-7dc2a54b277e",
        "outputId": "6ae71fef-fbd4-4589-e4bc-7851cdbdd6d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Spark Session Created Successfully.\n",
            "2. Data Loaded. Total Rows: 500\n",
            "3. Data Cleaned. Rows remaining: 500\n",
            "\n",
            "--- YOUR COLUMNS ---\n",
            "['farm_id', 'region', 'crop_type', 'soil_moisture_%', 'soil_pH', 'temperature_C', 'rainfall_mm', 'humidity_%', 'sunlight_hours', 'irrigation_type', 'fertilizer_type', 'pesticide_usage_ml', 'sowing_date', 'harvest_date', 'total_days', 'yield_kg_per_hectare', 'sensor_id', 'timestamp', 'latitude', 'longitude', 'NDVI_index', 'crop_disease_status']\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# PART 1: INIT & LOAD\n",
        "# ==========================================\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# 1. Initialize Spark Session\n",
        "# We use '*' to use all available cores to speed it up\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SmartFarming_Fix\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"1. Spark Session Created Successfully.\")\n",
        "\n",
        "# 2. Load Data\n",
        "# Make sure the filename matches EXACTLY what is in your folder\n",
        "file_path = \"Smart_Farming_Crop_Yield_2024.csv\"\n",
        "\n",
        "try:\n",
        "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "    print(f\"2. Data Loaded. Total Rows: {df.count()}\")\n",
        "\n",
        "    # 3. Clean Data (Drop duplicates and nulls)\n",
        "    df_clean = df.dropDuplicates().dropna()\n",
        "    print(f\"3. Data Cleaned. Rows remaining: {df_clean.count()}\")\n",
        "\n",
        "    # PRINT COLUMNS (Crucial for debugging)\n",
        "    print(\"\\n--- YOUR COLUMNS ---\")\n",
        "    print(df_clean.columns)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Could not load file. Make sure '{file_path}' is in the folder.\")\n",
        "    print(e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "080d5e96-b134-40cf-a28d-ea4059682ebe",
      "metadata": {
        "id": "080d5e96-b134-40cf-a28d-ea4059682ebe",
        "outputId": "01ac090b-2e8f-41e2-c66c-9c543c7bff15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- YOUR DATA COLUMNS ---\n",
            "['farm_id', 'region', 'crop_type', 'soil_moisture_%', 'soil_pH', 'temperature_C', 'rainfall_mm', 'humidity_%', 'sunlight_hours', 'irrigation_type', 'fertilizer_type', 'pesticide_usage_ml', 'sowing_date', 'harvest_date', 'total_days', 'yield_kg_per_hectare', 'sensor_id', 'timestamp', 'latitude', 'longitude', 'NDVI_index', 'crop_disease_status']\n",
            "\n",
            "SUCCESS MATCHED Categorical: ['region', 'crop_type', 'irrigation_type']\n",
            "SUCCESS MATCHED Numerical: []\n",
            "\n",
            "Running Pipeline... (This may take a moment)\n",
            "Pipeline Complete. Data is ready for ML.\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# PART 2: PIPELINE & PROCESSING (FIXED)\n",
        "# ==========================================\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
        "import sys\n",
        "\n",
        "# 1. COLUMN DETECTIVE\n",
        "# Let's see what columns are ACTUALLY in your dataframe\n",
        "print(\"--- YOUR DATA COLUMNS ---\")\n",
        "actual_columns = df_clean.columns\n",
        "print(actual_columns)\n",
        "\n",
        "# We define lists of \"Possible Names\" to catch variations (spaces vs underscores)\n",
        "possible_cats = [\n",
        "    'Region', 'region',\n",
        "    'Crop_Type', 'Crop Type', 'crop_type',\n",
        "    'Irrigation_Type', 'Irrigation Type', 'irrigation_type',\n",
        "    'Soil_Type', 'Soil Type', 'soil_type'\n",
        "]\n",
        "\n",
        "possible_nums = [\n",
        "    'Soil_Moisture', 'Soil Moisture', 'soil_moisture',\n",
        "    'Temperature_C', 'Temperature', 'temperature',\n",
        "    'Rainfall_mm', 'Rainfall', 'rainfall',\n",
        "    'Humidity', 'humidity',\n",
        "    'Fertilizer_Used_kg_per_ha', 'Fertilizer', 'fertilizer'\n",
        "]\n",
        "\n",
        "# Filter: We only keep the ones that actually exist in your CSV\n",
        "cat_cols = [c for c in possible_cats if c in actual_columns]\n",
        "num_cols = [c for c in possible_nums if c in actual_columns]\n",
        "\n",
        "print(f\"\\nSUCCESS MATCHED Categorical: {cat_cols}\")\n",
        "print(f\"SUCCESS MATCHED Numerical: {num_cols}\")\n",
        "\n",
        "# SAFETY CHECK: If no columns were found, stop here so we don't crash Spark\n",
        "if len(cat_cols) == 0 and len(num_cols) == 0:\n",
        "    print(\"\\nCRITICAL ERROR: No matching columns found!\")\n",
        "    print(\"Please manually update the 'possible_cats' and 'possible_nums' lists above to match your printed column names.\")\n",
        "    raise ValueError(\"Stopping execution: No features found.\")\n",
        "\n",
        "# 2. Build Pipeline Stages\n",
        "stages = []\n",
        "\n",
        "# Process Text Columns\n",
        "for c in cat_cols:\n",
        "    # Index: Text -> Number\n",
        "    s_indexer = StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"skip\")\n",
        "    # Encode: Number -> Vector\n",
        "    o_encoder = OneHotEncoder(inputCols=[f\"{c}_idx\"], outputCols=[f\"{c}_vec\"])\n",
        "    stages += [s_indexer, o_encoder]\n",
        "\n",
        "# Combine all features into one vector\n",
        "assembler_inputs = [f\"{c}_vec\" for c in cat_cols] + num_cols\n",
        "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"raw_features\")\n",
        "stages += [assembler]\n",
        "\n",
        "# Scale features (Standardize numbers)\n",
        "# Note: StandardScaler requires a vector. If we have features, this will now work.\n",
        "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"features\", withStd=True, withMean=True)\n",
        "stages += [scaler]\n",
        "\n",
        "# 3. Run Pipeline\n",
        "print(\"\\nRunning Pipeline... (This may take a moment)\")\n",
        "pipeline = Pipeline(stages=stages)\n",
        "model = pipeline.fit(df_clean)\n",
        "final_df = model.transform(df_clean)\n",
        "\n",
        "# Split Data for Training\n",
        "train_data, test_data = final_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "print(\"Pipeline Complete. Data is ready for ML.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "666c574c-a98d-454d-9823-23090202c0ab",
      "metadata": {
        "id": "666c574c-a98d-454d-9823-23090202c0ab",
        "outputId": "129a797a-ef63-4c92-de99-f70a86dc0bb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Model Training...\n",
            "Training Linear Regression...\n",
            "Linear Regression R2: -0.0189\n",
            "Training Random Forest...\n",
            "Random Forest R2: -0.0157\n",
            "Training GBT...\n",
            "GBT R2: -0.1223\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# PART 3: REGRESSION MODELS (FIXED)\n",
        "# ==========================================\n",
        "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "print(\"Starting Model Training...\")\n",
        "\n",
        "# CORRECT COLUMN NAME: lowercase 'y' based on your error log\n",
        "target_col = \"yield_kg_per_hectare\"\n",
        "\n",
        "# Define Evaluator (R2 Score)\n",
        "evaluator = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\", metricName=\"r2\")\n",
        "\n",
        "# 1. Linear Regression\n",
        "print(\"Training Linear Regression...\")\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=target_col)\n",
        "lr_model = lr.fit(train_data)\n",
        "lr_preds = lr_model.transform(test_data)\n",
        "print(f\"Linear Regression R2: {evaluator.evaluate(lr_preds):.4f}\")\n",
        "\n",
        "# 2. Random Forest (Usually better)\n",
        "print(\"Training Random Forest...\")\n",
        "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=target_col, numTrees=20)\n",
        "rf_model = rf.fit(train_data)\n",
        "rf_preds = rf_model.transform(test_data)\n",
        "print(f\"Random Forest R2: {evaluator.evaluate(rf_preds):.4f}\")\n",
        "\n",
        "# 3. Gradient Boosted Trees (GBT)\n",
        "print(\"Training GBT...\")\n",
        "gbt = GBTRegressor(featuresCol=\"features\", labelCol=target_col, maxIter=10)\n",
        "gbt_model = gbt.fit(train_data)\n",
        "gbt_preds = gbt_model.transform(test_data)\n",
        "print(f\"GBT R2: {evaluator.evaluate(gbt_preds):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04568fe1-5c08-4a9e-bf39-f72ed66f643e",
      "metadata": {
        "id": "04568fe1-5c08-4a9e-bf39-f72ed66f643e",
        "outputId": "27f54a8d-fcd0-4c24-9f70-3a2549f4bced"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exporting data for Tableau...\n",
            "Processing Regression Export...\n",
            "-> Saved: Tableau_Regression_Results.csv\n",
            "Running Clustering...\n",
            "-> Saved: Tableau_Cluster_Results.csv\n",
            "\n",
            "ALL DONE! Check your folder for the CSV files.\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# PART 4: EXPORT DATA (FIXED)\n",
        "# ==========================================\n",
        "import pandas as pd\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "print(\"Exporting data for Tableau...\")\n",
        "\n",
        "# 1. Export Regression Results (Using GBT predictions)\n",
        "# We convert to Pandas to save as a simple CSV\n",
        "# NOTE: We use the lowercase 'yield_kg_per_hectare' here too\n",
        "print(\"Processing Regression Export...\")\n",
        "reg_df = gbt_preds.select(\"region\", \"crop_type\", \"yield_kg_per_hectare\", \"prediction\").toPandas()\n",
        "reg_df[\"Residual\"] = reg_df[\"yield_kg_per_hectare\"] - reg_df[\"prediction\"]\n",
        "reg_df.to_csv(\"Tableau_Regression_Results.csv\", index=False)\n",
        "print(\"-> Saved: Tableau_Regression_Results.csv\")\n",
        "\n",
        "# 2. Quick Clustering for Visualization\n",
        "print(\"Running Clustering...\")\n",
        "# Re-define numerical columns based on your error log (using lowercase or correct names)\n",
        "# I am grabbing the ones visible in your error log:\n",
        "cluster_cols = ['soil_moisture_%', 'temperature_C', 'rainfall_mm', 'humidity_%']\n",
        "# Ensure these exist in the dataframe before using\n",
        "valid_cluster_cols = [c for c in cluster_cols if c in df_clean.columns]\n",
        "\n",
        "if valid_cluster_cols:\n",
        "    clust_assembler = VectorAssembler(inputCols=valid_cluster_cols, outputCol=\"cluster_features\")\n",
        "    clust_data = clust_assembler.transform(df_clean)\n",
        "\n",
        "    kmeans = KMeans(featuresCol=\"cluster_features\", k=3, seed=1)\n",
        "    km_model = kmeans.fit(clust_data)\n",
        "    clust_res = km_model.transform(clust_data)\n",
        "\n",
        "    # Export Clustering\n",
        "    clust_pdf = clust_res.select(\"region\", \"crop_type\", \"prediction\", *valid_cluster_cols).toPandas()\n",
        "    clust_pdf.to_csv(\"Tableau_Cluster_Results.csv\", index=False)\n",
        "    print(\"-> Saved: Tableau_Cluster_Results.csv\")\n",
        "else:\n",
        "    print(\"Skipping clustering export: Could not find matching sensor columns.\")\n",
        "\n",
        "print(\"\\nALL DONE! Check your folder for the CSV files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75dd6c6c-37b9-4321-a23b-124fe2d4e213",
      "metadata": {
        "id": "75dd6c6c-37b9-4321-a23b-124fe2d4e213"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}